{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c6fa5-2a72-4174-aaa1-bde07a4f4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b6917-adc9-416f-ac5d-1ba5efa581a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "เริ่มจากload environmentและตั้งค่าAPI_KEYก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b0084-5e13-4112-ab98-92c24bea874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environment\n",
    "load_dotenv(dotenv_path=r\"my_directory\\HUGGINGFACE_API_KEY.env\")\n",
    "\n",
    "# ตั้งค่า API Key ของ Hugging Face\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\") # ใส่คีย์ API ของคุณที่นี่\n",
    "\n",
    "if \"HUGGINGFACE_API_KEY\" in os.environ:\n",
    "    print(\"Environment variable loaded successfully!\")\n",
    "    print(\"HUGGINGFACE_API_KEY:\", os.environ[\"HUGGINGFACE_API_KEY\"])  # ตรวจสอบค่า (อย่าแชร์ค่า API Key จริง)\n",
    "else:\n",
    "    print(\"Failed to load environment variable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f8c3c-c451-465c-b7d6-07fb4cb32270",
   "metadata": {},
   "source": [
    "ทำการโหลดโมเดลLLMที่ต้องการใช้งานเข้ามา พร้อมทั้งระบุหน้าที่ของโมเดล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881a3b3-332c-494a-9a6d-c71946f59e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a73663-b115-4775-bc26-4668116ae3ce",
   "metadata": {},
   "source": [
    "ใช้WebBaseLoaderและSoupStrainerในการทำweb scraping ดึงข้อมูลจากเว็บไซต์ที่ต้องการและเก็บไว้ในตัวแปรdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86b019-45ce-4741-a483-0bbdab462883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ea9ac-d748-4800-bd3d-f31eb21d3109",
   "metadata": {},
   "source": [
    "ใช้ RecursiveCharacterTextSplitterเพื่อ\n",
    "1.ใช้แบ่งเอกสารขนาดใหญ่ให้เหมาะสมกับ Embeddings Models (เช่น OpenAI, Hugging Face)\n",
    "2.ใช้แบ่งข้อความก่อนนำไปประมวลผลกับ LLMs (Large Language Models)\n",
    "3.ใช้เตรียมข้อมูลก่อนนำเข้า Vector Database เช่น FAISS, Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332fdf9-f4e3-4df1-a002-509d3e4c7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ce433-0b59-4136-8699-2e67293665b8",
   "metadata": {},
   "source": [
    "ขั้นตอนต่อไปคือการนำ Chunks ข้อมูลเหล่านั้นมาเข้าสู่กระบวนการ Embeddings เพื่อแปลงข้อมูลให้อยู่ในรูปแบบของ Vector หลังจากนั้นเราจะเก็บข้อมูล Vectors ที่ได้ไว้ใน FAISS ซึ่งเป็น Vector store library ที่ถูกพัฒนามาสำหรับการทำ Similarity Search อย่างมีประสิทธิภาพ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996db803-6627-4e47-a7b8-6ac62aa75733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "#HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3da3-97f4-42a3-84ba-930afd3789a8",
   "metadata": {},
   "source": [
    "ต่อมา เราจะสร้างpromptเพื่อช่วยให้โมเดลเข้าใจบริบทผ่าน {context} ซึ่งcontextคือข้อมูลที่เราจะดึงมาจาก FAISS Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffff38-0733-4f22-a781-3b83d0242816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for easing to create prompt biolderpalte\n",
    "def make_llama_3_prompt(question, system=\"\", context=\"\"):\n",
    "  system_prompt = \"\"\n",
    "  if system != \"\":\n",
    "    system_prompt = (\n",
    "        f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system}\\n\\n\"\n",
    "        f\"context: {context}\\n\\n\"\n",
    "        f\"<|eot_id|>\\n\\n\"\n",
    "    )\n",
    "  prompt = (\n",
    "      f\"<|begin_of_text|>{system_prompt}\"\n",
    "      f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "      f\"{question}\\n\\n\"\n",
    "      f\"<|eot_id|>\\n\\n\"\n",
    "      f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" # header - assistant\n",
    "  )\n",
    "  return prompt\n",
    "\n",
    "system_prompt = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\n",
    "context = \"filler context\"\n",
    "user_prompt = \"What is Task Decomposition?\"\n",
    "prompt3 = make_llama_3_prompt(user_prompt, system_prompt, context)\n",
    "print(prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b78b87-ad47-404f-90a8-a46d1a582129",
   "metadata": {},
   "source": [
    "แต่เนื่องจากเราจะใช้ความสามารถของ LangChain Expression Language (LCEL) ในการทำ pipeline ด้วยการ chain ขั้นตอนต่างๆเข้าด้วย โดยให้ output ที่ได้จาก process ก่อนหน้า ไปเป็น input ให้กับ process ถัดไปเพื่อทำงานต่อแบบนี้ไปเรื่อยๆจนจบ process ดังนั้น เราต้องแปลง Function ด้านบนให้อยู่ในรูปแบบของ class ที่ implement Runnable interface เช่นเดียวกับ retriever และ llm โดยเราจะ implement method .invoke เพื่อให้เราสามารถ chain process เข้าหากันได้ด้วยความสามารถของ LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f161644-7225-4066-bf84-55a8744996a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Llama 3 Instruct uses a prompt template, with special tags used to indicate the user query and system prompt.\n",
    "# You can find the documentation on this [model card](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/#meta-llama-3-instruct).\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "class Llama3PromptRunnable(Runnable):\n",
    "    def __init__(self, system=\"\"):\n",
    "        super().__init__()\n",
    "        self.system = system\n",
    "\n",
    "    def invoke(self, inputs: dict, config=None) -> str:\n",
    "        question = inputs[\"question\"]\n",
    "        context = inputs[\"context\"]\n",
    "        # Create the system prompt if provided\n",
    "        system_prompt = \"\"\n",
    "        if self.system != \"\":\n",
    "            system_prompt = (\n",
    "                f\"<|start_header_id|>system<|end_header_id|>\\n\\n{self.system}\\n\\n\"\n",
    "                f\"context: {context}\\n\\n\"\n",
    "                f\"<|eot_id|>\\n\\n\"\n",
    "            )\n",
    "            prompt = (\n",
    "                f\"<|begin_of_text|>{system_prompt}\"\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"{question}\\n\\n\"\n",
    "                f\"<|eot_id|>\\n\\n\"\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" # header - assistant\n",
    "            )\n",
    "\n",
    "        # Return the formatted prompt\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc13fd-7b77-4f65-ad5c-cccdfeab2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "llama_prompt = Llama3PromptRunnable(system=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\")\n",
    "formatted_prompt = llama_prompt.invoke({\"context\": \"filler context\", \"question\": \"What is Task Decomposition?\"})\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b3e3a-9a7c-4d37-9074-19645f503a3d",
   "metadata": {},
   "source": [
    "หลังจากที่เราเตรียมทุกอย่างพร้อมแล้ว ไม่ว่าจะเป็น llm, retrieval, และ prompt ขั้นตอนถัดไปเราจะร้อยสามสิ่งนี้เข้าด้วยกัน ด้วยความสามารถของ LCEL Runnable โดยเราต้องใช้RunnableLambdaเพื่อดึงคำตอบที่โมเดลเจนออกมาแล้วส่งผ่านStrOutputParserเพื่อช่วยจัดการกับคำตอบให้หน้าตาเข้าถึงง่ายขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036d6ab-1ff5-40bc-b94d-2f990b7784e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} #output {\"context\":\"XX\", \"question\": \"YY\"}\n",
    "    | llama_prompt #take input from previous runable, {\"context\":\"XX\", \"question\": \"YY\"}, and return prompt str\n",
    "    | pipe #take input from previous runable, prompt str, to generate the answer\n",
    "    | RunnableLambda(lambda x: x[0][\"generated_text\"])  # ดึงค่า generated_text ออกมา\n",
    "    | StrOutputParser() #take input from previous runable, answer, to parser output\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6e82e-6f69-491c-9a00-922edc5c011a",
   "metadata": {},
   "source": [
    "ต่อมาเราจะสร้างHistory chatเพื่อให้โมเดลสามารถจดจำcontextของคำถามเดิมได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255837c-9e76-45b7-8ae5-7a13875ab12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)  # แปลงเป็น Runnable\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever, return_source_documents=True)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "query = \"What is Task Decomposition?\"\n",
    "response = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf165fa-647f-42a4-abe9-fd7a9597f9d0",
   "metadata": {},
   "source": [
    "ทีนี้เราลองถามต่อจาก context เดิมดังนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5e656-580b-40f0-a63a-bd9fecd71c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, response[\"answer\"])]\n",
    "\n",
    "query = \"What are the challenges?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
